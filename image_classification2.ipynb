# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""

import matplotlib.pyplot as plt
import os
import seaborn as sns
import pandas as pd
import numpy as np

# example of loading the mnist dataset
from keras.datasets import cifar10
from keras.models import Model
from keras.models import Sequential
from keras.layers import MaxPooling2D, Conv2D, Dropout,Dense,Flatten, Concatenate, BatchNormalization,Lambda, Activation
import keras
from keras import backend as K
from keras.optimizers import SGD,RMSprop, Adam, Adadelta
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator

from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping
from keras.layers import Input

from sklearn.utils import shuffle
from sklearn.utils import class_weight
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix,classification_report
from random import randint
from IPython.display import SVG
import matplotlib.gridspec as gridspec



def model_conv():
    '''
    Architektura modelu'''
    
    inputs = Input((size, size, 3))

    c1 = Conv2D(4, (3, 3),  padding = 'same')(inputs)
    c1 = BatchNormalization()(c1) 
    c1 = Activation('relu')(c1)
    c1 = Conv2D(4, (3, 3),  padding = 'same')(c1)
    c1 = BatchNormalization()(c1) 
    c1 = Activation('relu')(c1)

    c1 = MaxPooling2D(2, 2)(c1)

    c2 = Conv2D(16, (3, 3),  padding = 'same')(c1)
    c2 = BatchNormalization()(c2) 
    c2 = Activation('relu')(c2)
    c2 = Conv2D(16, (3, 3),  padding = 'same')(c2)
    c2 = BatchNormalization()(c2) 
    c2 = Activation('relu')(c2)

    c2 = MaxPooling2D(2, 2)(c2)


    c3 = Conv2D(32, (3, 3),  padding = 'same')(c2)
    c3 = BatchNormalization()(c3) 
    c3 = Activation('relu')(c3)
    c3 = Conv2D(32, (3, 3),  padding = 'same')(c3)
    c3 = BatchNormalization()(c3) 
    c3 = Activation('relu')(c3)

    c3 = MaxPooling2D(2, 2)(c3)

    c4 = Conv2D(64, (3, 3),  padding = 'same')(c3)
    c4 = BatchNormalization()(c4) 
    c4 = Activation('relu')(c4)
    c4 = Conv2D(64, (3, 3),  padding = 'same')(c4)
    c4 = BatchNormalization()(c4) 
    c4 = Activation('relu')(c4)

    c4 = MaxPooling2D(2, 2)(c4)

    c5 = Conv2D(128, (3, 3),  padding = 'same')(c4)
    c5 = BatchNormalization()(c5) 
    c5 = Activation('relu')(c5)
    c5 = Conv2D(128, (3, 3),  padding = 'same')(c5)
    c5 = BatchNormalization()(c5) 
    c5 = Activation('relu')(c5)

    c5 = MaxPooling2D(2, 2)(c5)

    '''c6 = Conv2D(256, (3, 3),  padding = 'same')(c5)
    c6 = BatchNormalization()(c6) 
    c6 = Activation('relu')(c6)
    c6 = Conv2D(256, (3, 3),  padding = 'same')(c6)
    c6 = BatchNormalization()(c6) 
    c6 = Activation('relu')(c6)

    c6 = MaxPooling2D(2, 2)(c6)'''

    f1 = Flatten()(c5)
    d1 = Dense(265, activation = 'relu')(f1)
    d3 = Dropout(0.2)(d1)
    #d3 = Dense(128, activation = 'relu')(d3)
    #d3 = Dropout(0.2)(d3)
    output = Dense(10, activation = 'softmax')(d3)


    model = Model(inputs = [inputs], outputs = [output])

    model.summary()
    return model


# load dataset
(trainX, trainy), (testX, testy) = cifar10.load_data()

#training parameters
epochs = 10
batch_size = 100
size = trainX.shape[1]

#Odwołanie się do modelu 
model=model_conv()

#Normalizacja zdjęć
trainX = np.asarray(trainX)/255
testX = np.asarray(testX)/255

#podział na zbiór uczący i walidacyjny
trainX, valX, trainy, valy = train_test_split(trainX, trainy, random_state=0, test_size=0.1, stratify=trainy)

#kategoryzacja label
trainy = keras.utils.to_categorical(trainy, 10)
valy = keras.utils.to_categorical(valy, 10)
testy = keras.utils.to_categorical(testy, 10)

'''#wagowanie klas ze względu na ich liczebnosc
class_weights = class_weight.compute_class_weight(
               'balanced',
                np.unique(trainy), 
                trainy)'''

#wagowanie klas manualnie
#class_weights={0:10, 1:1, 2:0.5, 3:2, 4:10, 5:4, 6:2, 7:4, 8:8, 9:1, 10:10}

#funkcja wizalizuąca losowe wyniki klasyfikacji
dict = {0:'0', 1:'1', 2:'2', 3:'3', 4:'4', 5:'5', 6:'6', 7:'7', 8:'8', 9:'9'}
def showOrigDec(orig, dec, num=2):  ## function used for visualizing original and reconstructed images of the autoencoder model
    n = num
    plt.figure(figsize=(20, 4))

    for i in range(n):
        # display original
        ax = plt.subplot(2, n, i+1)
        plt.imshow(orig[300*i].reshape(til,til, 3))
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)

        # display reconstruction
        ax = plt.subplot(2, n, i +1 + n)
        plt.imshow(dec[300*i].reshape(til,til, 3))
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    plt.show()
        
def show_test(m, d):  ## function used for visualizing the predicted and true labels of test data
    plt.figure(figsize =(40,8))
    for i in range(5):
        ax = plt.subplot(1, 5, i+1)
        test_image = np.expand_dims(d[101*i+5], axis=0)
        test_result = m.predict(test_image)
        plt.imshow(testX[101*i+5])
        index = np.argsort(test_result[0,:])
        plt.title("Pred:{}, True:{}".format(dict[index[1]], dict[testy[101*i+5][1]]))
    plt.show()
    
#funkcja generująca raport wyniku klasyfikacji
def report(predictions): ## function used for creating a classification report and confusion matrix
    cm=confusion_matrix(testy.argmax(axis=1), predictions.argmax(axis=1))
    print("Classification Report:\n")
    cr=classification_report(testy.argmax(axis=1),
                                predictions.argmax(axis=1), 
                                target_names=list(dict.values()))
    print(cr)
    plt.figure(figsize=(8,8))
    sns.heatmap(cm, annot=True, xticklabels = list(dict.values()), yticklabels = list(dict.values()), fmt="d",annot_kws={"size": 20})
    
#funkcja kosztow, która  koncentruje się na trudnych do zweryfikowania elementach
def categorical_focal_loss(gamma=2., alpha=.25):
    """
    Softmax version of focal loss.
           m
      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)
          c=1
      where m = number of classes, c = class and o = observation
    Parameters:
      alpha -- the same as weighing factor in balanced cross entropy
      gamma -- focusing parameter for modulating factor (1-p)
    Default value:
      gamma -- 2.0 as mentioned in the paper
      alpha -- 0.25 as mentioned in the paper
    References:
        Official paper: https://arxiv.org/pdf/1708.02002.pdf
        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy
    Usage:
     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=["accuracy"], optimizer=adam)
    """
    def categorical_focal_loss_fixed(y_true, y_pred):
        """
        :param y_true: A tensor of the same shape as `y_pred`
        :param y_pred: A tensor resulting from a softmax
        :return: Output tensor.
        """

        # Scale predictions so that the class probas of each sample sum to 1
        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)

        # Clip the prediction value to prevent NaN's and Inf's
        epsilon = K.epsilon()
        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)

        # Calculate Cross Entropy
        cross_entropy = -y_true * K.log(y_pred)

        # Calculate Focal Loss
        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy

        # Sum the losses in mini_batch
        return K.sum(loss, axis=1)

    return categorical_focal_loss_fixed

#Zapuszczenie modelu w zależnsci od parametru o uwzględnienie augmentacji danych
def run_conv_model(data_aug):
    er = EarlyStopping(monitor='val_acc', patience=10, restore_best_weights=True)
    lr = ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=5, min_delta=0.0001)
    callbacks = [er, lr]
    
    if not data_aug:
        history =model.fit(trainX, trainy, batch_size=batch_size,
                                 epochs=epochs,
                                 verbose=1, callbacks=callbacks,
                                 validation_data=(valX,valy))
#                                class_weight=class_weights)
    else:
        train_datagen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
        train_set_ae = train_datagen.flow(trainX, trainy, batch_size=batch_size)

        validation_datagen = ImageDataGenerator()
        validation_set_ae = validation_datagen.flow(valX, valy, batch_size=batch_size)
        
        history = model.fit_generator(train_set_ae,
                                           epochs=epochs,
                                           steps_per_epoch=np.ceil(trainX.shape[0]/batch_size),
                                           verbose=1, callbacks=callbacks,
                                           validation_data=(validation_set_ae),
                                           validation_steps=np.ceil(valX.shape[0]/batch_size))
#                                            class_weight=class_weights)
        
        return history
#Inicjalizacja modelu
opt_rms = Adadelta()
model.compile(loss=[categorical_focal_loss(alpha=.05, gamma=2)],
                   optimizer=opt_rms,
                   metrics=['accuracy'])

history=run_conv_model(1)

#Wykresy dokładnosci i strat
acc =history.history['accuracy']
val_acc =history.history['val_accuracy']
loss =history.history['loss']
val_loss=history.history['val_loss']

plt.figure(figsize=(8,8))
plt.subplot(2, 1, 1)
plt.plot(acc, label='Training Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.legend(loc='upper left')
plt.ylabel('Accuracy')
plt.ylim([min(plt.ylim()),max(plt.ylim())])
plt.title('Training and Validation Accuracy')

plt.subplot(2, 1, 2)
plt.plot(loss, label='Training Loss')
plt.plot(val_loss, label='Validation Loss')
plt.legend(loc='lower left')
plt.ylabel('Cross Entropy')
plt.ylim([min(plt.ylim()),max(plt.ylim())])
plt.title('Training and Validation Loss')
plt.xlabel('epoch')
plt.show()
    
#sprawdzeniewyniku klasyfikacji
show_test(model, testX)

#predykcja danych i wyników
predictions = model.predict(testX)
report(predictions)

#zapis modelu i wag
# Save the weights
model.save_weights('model_weights.h5')

# Save the model architecture
with open('model_architecture.json', 'w') as f:
    f.write(model.to_json())
    
    
#wczytanie modelu
#load model
from keras.models import model_from_json

# Model reconstruction from JSON file
with open('model_architecture.json', 'r') as f:
    model = model_from_json(f.read())

# Load weights into the new model
model.load_weights('model_weights.h5')



